[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/EqWXl6Ay)
# Gerrymandering-Safe Redistricting with Machine Learning

*Authored by Alexander Baumguartner, Vikram Seshadri, Akaash Babu*

## Project Overview
This project aims to use a Markov chain Monte Carlo (MCMC) algorithm to generate ethical and gerrymandering-free electoral district boundaries. The model takes into account several fairness criteria, including:
- **Voting Rights Act (VRA) Compliance**: Ensuring minority voting power is not diluted.
- **Compactness**: Districts should be geographically compact and contiguous.
- **Communities of Interest**: Defined using residential-employment data to reflect real-world community structures.
- **Population Equality**: Districts should have roughly equal populations.

We build our shell on the structure proposed by Fifeld et al. (2019). Our code is contained in the "Final_Model.ipynb" file, with additional code for data cleaning located in the files titled DataCleaner. All branches have been merged into this one ("main"). 

## Discussion of Algorithm
Our goal in developing an algorithm is to put forth an abstract, accessible, modular framework that can easily be configured for various suitable redistricting applications. 
Explicitly, our goal is not to find a singular algorithm, nor a “determinative solution” for redistricting. Not only does this not exist for many jurisdictions (as proven by individuals far smarter than us), but it becomes quite difficult to define a solution that is absolutely best for all applications. Instead, redistricting is often conducted with incentives, wishes, and outcomes being evaluated on a case-by-case basis, making it useless to put forward one “correct” way of doing so algorithmically.

# Overall Structure
We based the foundation of our algorithm on work conducted by Fifeld et al. (2019), who demonstrate redistricting as a “graph-cut” problem and the feasibility of utilizing a Markov chain Monte Carlo (MCMC) model to descend based on a set of target criteria. Fifeld et al. (2019) also provide a basic algorithm and code of their own using their application of this concept. Although we chose to utilize the basic shell of a MCMC model applied to a graph-cut problem, we decided to construct an algorithm of our own in line with our wishes for modularity, simplicity, adaptability, and compliance with legal requirements that often inhibit the use of such algorithms in practice.

In redefining redistricting as a “graph cut” problem, Fifeld et al. (2019) demonstrates the use of representing a state as a set of “nodes” representing geographical units, and edges between nodes implying congruence. Districts, in essence, remove edges (“cutting” them) to create smaller groups of nodes. Formulating districts in this fashion not only makes it easy to demonstrate congruence (a surprisingly difficult task in other models, and one explicitly required in practically every application of redistricting), but makes the optimization process far more computationally efficient. It is also fairly easily implemented by the networkx library in Python. 

The Markov chain Monte Carlo (MCMC) algorithm we implemented is a probabilistic method designed to explore the vast solution space of possible redistricting plans by iteratively proposing and evaluating changes to district boundaries. At its core, the algorithm operates by scoring each proposed map based on a set of four criteria—compactness, community preservation, population parity, and compliance with the Voting Rights Act (VRA). Using these scores, the algorithm determines whether to accept a proposed change, favoring maps with higher scores but occasionally also accepting lower-scoring maps. This latter step is key, as it allows us to escape local minima and have a true exploration of the sample space. This iterative process allows the algorithm to converge toward district configurations that better balance the competing priorities of fairness, legality, and practicality. By structuring the problem as a Markov chain, the algorithm ensures that each proposed map depends only on the current state, creating a flexible yet systematic method for navigating the complexities of redistricting. 

# Initializing Districts
The algorithm begins by initializing districts through a systematic and iterative process designed to ensure both contiguity and reasonable population balance. The initialization starts by selecting a random block as a "seed" for each district and expanding the district through a flood-fill by adding neighboring blocks until its population approaches the target (typically the total population of the jurisdiction divided by the number of desired districts). This expansion prioritizes contiguity, as blocks are only added if they share a boundary with the district, and the “flood-fill” approach makes the district somewhat contiguous. After the initial assignment, any unassigned blocks are allocated to districts based on adjacency or, if necessary, by redistributing blocks from overpopulated districts. It was discovered that extremely small initial districts had a potential to “decay” at their small populations given certain model conditions if left unadulterated. To prevent this, the algorithm also handles cases where a district's population falls below a certain threshold (defined by a hyperparameter as a percentage of the target population) by redistributing blocks from neighboring districts (that have a minimum population defined by another hyperparameter), ensuring all districts are sufficiently populated to avoid this decay. This initialization provides a feasible starting point for the MCMC process, allowing subsequent iterations to refine the district boundaries further. The initialization of districts was performed in this manner (instead of simply utilizing a previous boundary or a similar approach) to ensure maximum applicability. 
A point of emphasis both in this initial algorithm and all future steps was congruity - any configuration or assignment that violated the congruity of any district or subgraph was immediately declared invalid and written back.  

# Generating New Proposals
From this initial map, new maps were generated and compared to the existing one using the aforementioned MCMC algorithm. The generation of new maps was done by randomly selecting a district, and then randomly selecting a node within that district that had an edge outside of the district. This node would then be swapped with one of the districts it bordered, to create a new map. A key insight in this regard regarding the overweighting of nodes with more out-of-district edges was brought to us by Dr. Brian Haidet’s implementation of Fifeld et al. (2019), in addition to insights regarding the scoring of compactness and a “phasing” approach to overall scoring discussed later (Haidet 2019). 

# Scoring
Scoring was conducted based on a weighted average of four factors: VRA compliance, population parity, compactness, and community integrity. Each of these scores was scaled to output a number from 0 to 1. VRA compliance was straightforward, and afforded scores in line with how many districts were majority-minority of a particular race. Population parity was based on mean deviations from the target population raised to the fourth power to harshly penalize deviations further away from the target. Compactness was defined for each district as the scaled mean squared distances of points away from the center of population of each district, with scores being adjusted based on district size, and a harmonic mean forming the composite to disincentivize particularly low scores. We originally utilized a modified “Polsby-Popper” score, which descended into extremely unfriendly looking maps, but found literature describing mean squared distances as being a better approach (Haidet 2019). 

“Community integrity” was our attempt at approximating “communities of interest.” As aforementioned in the “fairness” section, “communities of interest” is a term that is fairly difficult to define. Concerningly, many of the other community integrity metrics we considered (such as utilizing cell phone movement data) were extremely unfeasible to repeatedly re-calculate at each step of the MCMC algorithm. As a result, we utilized a measure based on employment and residence data. Believing congruent communities would tend to have populations who both worked and lived within that community, we utilized a composite weighted score of the percent of those who live in a district who also work in that district, and the percent of those who work in a district who also live in a district. The former was weighted heavier, as the latter resulted in distortions around downtown/industrial areas. Data was collected from the Census’s LEHD Origin-Destination Employment Statistics (LODES) survey. 

# Phasing
Another key insight was the need to have multiple “phases” of descent, each with different scoring weights and temperatures for the MCMC algorithm (Haider 2019). In the first phase of the algorithm, population parity and compactness were prized over all, and temperature was low. This served almost as a continuation of the initialization algorithm, and a necessary step to ensure reasonable districts. The second phase had a higher weight on community preservation and greater temperature, allowing greater deviation in compactness and population parity to allow adequate exploration. The third phase emphasized legal compliance, with a focus on population parity and VRA compliance to ensure the output would be compliant with these more stringent requirements. The weights and temperatures for each phase are adjustable, and for further application, should be changed in-line with the particular needs, incentives, and wishes of those conducting a redistricting. 

# Efficiency 
In addition to refining our scoring processes, our main effort following the initialization of the model was to make it as efficient as possible, and feasible to run in a short amount of time. We are confident in our achievement of this task—while iterations took about 10 seconds each when we began, by the end, we were able to perform each iteration in just 0.9 seconds on our relatively pedestrian personal laptops, and each iteration achieved a moderate amount of exploration/descent. 

## Datasets Utilized

1. Census Tract Data (TIGER/Line Shapefiles) - we chose to utilize census tracts instead of more granular precincts or blocks for efficiency's sake. Contains shapefiles, population information, racial information, and other census characteristics - obtained from the US Census Bureau
2. LODES Data - also obtained from the Census Bureau

## Directions

## References/Sources

Fifield, Benjamin, Higgins Michael, Kosuke Imai, and Alexander Tarr. 2020. “Automated Redistricting Simulation Using Markov Chain Monte Carlo.” Journal of Computational and Graphical Statistics 29 (4): 715–28. doi:10.1080/10618600.2020.1739532.

Haidet, Brian. "Algorithmic Redistricting: Elections made-to-order." YouTube video, 26:47. Posted by AlphaPhoenix, June 21, 2021. https://www.youtube.com/watch?v=Lq-Y7crQo44&t=1254s.